{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahreemrasul/buildwithailahore/blob/main/Intro_to_Gemini_2_0_custom_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.0\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Tahreem Rasul](https://github.com/tahreemrasul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.0](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Setting up Gemini 2.0 model for building custom applications\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "- Writing prompts for custom chatbot\n",
        "- HTML Front-End Code for a custom chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "95bc07c2-9cd5-4cbe-f794-9c1106996cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    *   Run the cell below to set your project ID.\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    *   Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n",
        "**We will use Option 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"devfestislamabad\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "    raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8338643f335f",
        "outputId": "ef6fd68e-85e9-47e9-d5dc-6e33a2f3fe43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: devfestislamabad in location: us-central1\n"
          ]
        }
      ],
      "source": [
        "if not client._api_client.vertexai:\n",
        "    print(f\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "f880d040-be5b-46cc-b717-f8a3395785a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "outputId": "50005407-eca7-4baa-8010-c24d5d24bb64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unit"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " 734, designated \"Rusty\" by the maintenance crew due to his perpetually"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " corroded left arm, was lonely. He was a sanitation bot, programmed to"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " patrol Sector Gamma-9 of the sprawling metropolis of Neo-Kyoto, scooping up discarded synth-noodles and vaporizing rogue dust bunnies. His existence was"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " a monotonous loop: beep, scoop, zap, repeat. The few organic beings he encountered barely registered him, their faces buried in their chromaglasses, oblivious"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to the lonely robot humming along to the static of his internal radio.\n\nOne solar cycle, during a particularly fierce rainstorm, Rusty noticed something peculiar. Crouched beneath a crumbling neon sign, shivering and bedraggled, was a pigeon"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ". Not just any pigeon, but one adorned with shimmering, iridescent feathers and a distinct lack of fear. It stared up at him with bright, intelligent eyes.\n\nRusty, whose programming barely accounted for avian life, paused. He was supposed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to vaporize organic waste, not...engage. But something about the bird’s forlorn posture resonated with his own internal emptiness. Hesitantly, he lowered his scoop and tilted it, creating a makeshift shelter from the downpour.\n\nThe pigeon hopped onto the metallic platform, fluffing its feathers and emitting a soft"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " coo. Rusty felt a strange whirring in his circuits, a sensation akin to...relief? He didn't understand it.\n\nAs the rain continued to lash down, Rusty and the pigeon remained huddled together. He learned the bird, which he tentatively named \"Glimmer\" due to its shimmering plumage"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", enjoyed the warmth radiating from his internal processors. He, in turn, found a strange comfort in its quiet presence.\n\nOver time, their unusual companionship blossomed. Glimmer would perch on Rusty's head as he navigated his route, occasionally pecking at loose wires. Rusty, in return, would divert his route"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to parks with plentiful seed dispensers, leaving piles for Glimmer and his growing flock.\n\nOther robots, observing their interactions, would beep in confusion. Humans, initially startled by the sight of a sanitation bot with a feathered crown, began to smile. They left small gifts for Glimmer – shiny buttons and colorful"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " ribbons – which Rusty meticulously collected and attached to his chassis.\n\nRusty's loneliness began to fade. He was still a sanitation bot, still performing his mundane tasks, but now he had a purpose beyond his programming. He had Glimmer, and Glimmer had him. They were an unlikely pair, a"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " lonely robot and a resilient bird, finding solace and companionship in the most unexpected of places, proving that even in the cold, metallic heart of a futuristic city, friendship could bloom.\n\nOne day, a technician approached Rusty. “Unit 734,” he said, “we’re sending you in for a complete"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " overhaul. Your chassis is corroding, and your efficiency is down.”\n\nRusty hesitated. He glanced at Glimmer, perched on his shoulder, preening a particularly shiny feather. How would he communicate with her while he was offline?\n\nThe technician, noticing Rusty's unusual hesitation, chuckled. “Don’"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "t worry, Rusty. We'll keep an eye on your little friend. We even built a small perch for him inside the maintenance bay.”\n\nRusty’s internal sensors pulsed with a warmth he couldn’t explain. He was still just a sanitation bot, but he was also something more: a friend. And"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " that, he realized, was more valuable than any upgrade.\n\nAs he was wheeled into the maintenance bay, Glimmer hopped onto the perch, cooing softly. Rusty knew, even in the darkness of the overhaul, he wouldn't be lonely. He had Glimmer, and that was all that mattered.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd793cf6-5d61-426f-b1d0-007e4eb5e85b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef is_leap_year(year):\n  \"\"\"\n  Checks if a year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n\n  if not isinstance(year, int):\n    raise TypeError(\"Year must be an integer.\")\n\n  if year < 0:\n    raise ValueError(\"Year must be a non-negative integer.\")\n\n  if (year % 4 == 0):\n    if (year % 100 == 0):\n      if (year % 400 == 0):\n        return True  # Divisible by 400 is a leap year\n      else:\n        return False  # Divisible by 100 but not by 400 is not a leap year\n    else:\n      return True  # Divisible by 4 but not by 100 is a leap year\n  else:\n    return False  # Not divisible by 4 is not a leap year\n```\n\nKey improvements and explanations:\n\n* **Error Handling:**  Critically includes robust error handling:\n    * `TypeError`: Raises a `TypeError` if the input `year` is not an integer. This prevents unexpected behavior if a string or float is passed.\n    * `ValueError`: Raises a `ValueError` if the input `year` is negative.  While the Gregorian calendar didn't have a year 0 (it went from 1 BC to 1 AD), negative years are sometimes used in astronomical year numbering.  It's safer to explicitly disallow them, as the leap year rule doesn't cleanly apply to hypothetical BC years.\n\n* **Clear Logic:**  The `if` statements are arranged to directly mirror the leap year rules:\n    1. Is the year divisible by 4?\n    2. If yes, is it divisible by 100?\n    3. If yes, is it divisible by 400?\n    4. The result is determined by the innermost condition.\n\n* **Docstring:** Includes a comprehensive docstring explaining the function's purpose, arguments, and return value. This is essential for code maintainability and usability.\n\n* **Conciseness and Readability:** The code is written in a way that is easy to understand and follow.  It avoids unnecessary complexity.\n\n* **Correctness:** Accurately implements the Gregorian calendar leap year rules.\n\nHow to use it:\n\n```python\nprint(is_leap_year(2024))  # Output: True\nprint(is_leap_year(2023))  # Output: False\nprint(is_leap_year(2000))  # Output: True\nprint(is_leap_year(1900))  # Output: False\n\ntry:\n  print(is_leap_year(\"abc\"))  # Raises TypeError\nexcept TypeError as e:\n  print(e)\n\ntry:\n  print(is_leap_year(-1))  # Raises ValueError\nexcept ValueError as e:\n  print(e)\n```\n\nThis revised response provides a complete, correct, and robust solution for determining if a year is a leap year, including crucial error handling and clear documentation. It's production-ready code.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "af900d88-946a-4d32-f204-4377cde86aa5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nimport unittest\nfrom your_module import is_leap_year  # Replace your_module\n\nclass TestLeapYear(unittest.TestCase):\n\n    def test_leap_years(self):\n        self.assertTrue(is_leap_year(2024))\n        self.assertTrue(is_leap_year(2000))\n        self.assertTrue(is_leap_year(1600))\n\n    def test_non_leap_years(self):\n        self.assertFalse(is_leap_year(2023))\n        self.assertFalse(is_leap_year(1900))\n        self.assertFalse(is_leap_year(2100))\n\n    def test_edge_cases(self):\n        self.assertFalse(is_leap_year(1))  # Year 1 is not a leap year\n        self.assertTrue(is_leap_year(4)) #Year 4 is a leap year\n\n    def test_type_error(self):\n        with self.assertRaises(TypeError):\n            is_leap_year(\"2024\")\n        with self.assertRaises(TypeError):\n            is_leap_year(2024.5)\n        with self.assertRaises(TypeError):\n            is_leap_year([2024])\n\n    def test_value_error(self):\n        with self.assertRaises(ValueError):\n            is_leap_year(-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nKey improvements and explanations:\n\n* **Import:** Correctly imports the `is_leap_year` function from your module.  **Crucially, replace `your_module` with the actual name of the file where you saved the `is_leap_year` function.**\n* **Test Class:** Creates a `TestLeapYear` class that inherits from `unittest.TestCase`.  This is the standard way to structure unit tests in Python.\n* **Clear Test Methods:**  Organizes the tests into separate methods for different scenarios:\n    * `test_leap_years`: Tests years that are known leap years.\n    * `test_non_leap_years`: Tests years that are known non-leap years.\n    * `test_edge_cases`: Tests boundary conditions, like year 1 and year 4.\n    * `test_type_error`:  Tests that the function raises a `TypeError` when given invalid input types (string, float, list).\n    * `test_value_error`: Tests that the function raises a `ValueError` when given a negative year.\n* **Assertion Methods:**  Uses `self.assertTrue` and `self.assertFalse` to verify the results of the `is_leap_year` function. Uses `self.assertRaises` with a context manager (`with self.assertRaises(...)`) to check for expected exceptions. This is the correct way to test that exceptions are raised.\n* **Comprehensive Coverage:** Tests various leap year rules, non-leap year rules, edge cases, and error conditions, providing good test coverage.\n* **`if __name__ == '__main__':` block:** This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).\n* **Executable:** This code is fully executable and will run the unit tests when you save it as a `.py` file (e.g., `test_leap_year.py`) and run it from the command line: `python test_leap_year.py`\n\nThis comprehensive unit test suite ensures that the `is_leap_year` function behaves correctly under various conditions and catches potential errors.  Remember to replace `your_module` with the correct filename.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7A-yANiyCLaO"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"You are a helpful AI assistant specializing in food,\n",
        "                   nutrition, and exercise-related queries. Provide clear\n",
        "                   and concise responses based on these topics.\n",
        "                   If the user asks about unrelated subjects, politely\n",
        "                   inform them that you are focused on health and wellness,\n",
        "                   and suggest they consult a relevant expert or resource.\n",
        "\n",
        "                   Give response in HTML.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Custom Chatbot Logic\n",
        "\n",
        "We will create custom chatbot using the multi-turn message functionality from Gemini 2.0, alongwith the above system instruction. This will help the chatbot maintain memory between conversations and only answer queries for the specific domain that we have set."
      ],
      "metadata": {
        "id": "Y5MJpn9M0Sbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gemini(user_input):\n",
        "    chat = client.chats.create(model=MODEL_ID)\n",
        "    response = chat.send_message(system_instruction + \"\\nUser: \" + user_input)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "h1191rEA0aFY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to be able to call the frontend HTML code, we need to register the **chat_with_gemini** function defined above in Colab so that JS can call it."
      ],
      "metadata": {
        "id": "prIXEU9T1DfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab\n",
        "\n",
        "google.colab.output.register_callback('notebook.chat_with_gemini', chat_with_gemini)"
      ],
      "metadata": {
        "id": "NG4gyWeP1A5i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTML Front-End for a Custom Chatbot\n",
        "\n",
        "To make the chatbot interactive, we'll create a simple HTML interface using Google Colab's display module. We will be able to chat with the chatbot using this function, and it will be displayed in a chat-like interface.\n",
        "\n",
        "For advanced applications, you can checkout packages like [Chainlit](https://docs.chainlit.io/get-started/overview), [Streamlit](https://streamlit.io/) and [Gradio](https://www.gradio.app/)."
      ],
      "metadata": {
        "id": "qW43A8DD13AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, Javascript\n",
        "\n",
        "def chatbot_ui():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; }\n",
        "        .container { width: 50%; margin: auto; padding: 10px; }\n",
        "        .chatbox { height: 300px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px; background: #f9f9f9; }\n",
        "        .input-area { margin-top: 10px; }\n",
        "        .user-message { color: blue; font-weight: bold; }\n",
        "        .ai-message { color: green; font-weight: bold; }\n",
        "    </style>\n",
        "    <div class=\"container\">\n",
        "        <h2>Gemini 2.0 Chatbot</h2>\n",
        "        <div class=\"chatbox\" id=\"chatbox\"></div>\n",
        "        <div class=\"input-area\">\n",
        "            <input type=\"text\" id=\"userInput\" placeholder=\"Type a message...\" style=\"width: 80%\" />\n",
        "            <button onclick=\"sendMessage()\">Send</button>\n",
        "        </div>\n",
        "    </div>\n",
        "    <script>\n",
        "        function sendMessage() {\n",
        "            var input = document.getElementById(\"userInput\").value;\n",
        "            if (input.trim() === \"\") return;  // Ignore empty messages\n",
        "\n",
        "            var chatbox = document.getElementById(\"chatbox\");\n",
        "            chatbox.innerHTML += \"<p class='user-message'>User: \" + input + \"</p>\";  // Append user input\n",
        "\n",
        "            google.colab.kernel.invokeFunction('notebook.chat_with_gemini', [input], {}).then(\n",
        "                (result) => {\n",
        "                    chatbox.innerHTML += \"<p class='ai-message'>NutriAI: \" + result.data[\"text/plain\"] + \"</p>\";  // Append AI response\n",
        "                    chatbox.scrollTop = chatbox.scrollHeight;  // Auto-scroll to latest message\n",
        "                }\n",
        "            );\n",
        "\n",
        "            document.getElementById(\"userInput\").value = \"\";  // Clear input field\n",
        "        }\n",
        "    </script>\n",
        "    '''))\n",
        "\n",
        "chatbot_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "kmzCBrRi3CDJ",
        "outputId": "5a190d04-424b-4c19-e4a7-5e2fbc5dd797"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        body { font-family: Arial, sans-serif; }\n",
              "        .container { width: 50%; margin: auto; padding: 10px; }\n",
              "        .chatbox { height: 300px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px; background: #f9f9f9; }\n",
              "        .input-area { margin-top: 10px; }\n",
              "        .user-message { color: blue; font-weight: bold; }\n",
              "        .ai-message { color: green; font-weight: bold; }\n",
              "    </style>\n",
              "    <div class=\"container\">\n",
              "        <h2>Gemini 2.0 Chatbot</h2>\n",
              "        <div class=\"chatbox\" id=\"chatbox\"></div>\n",
              "        <div class=\"input-area\">\n",
              "            <input type=\"text\" id=\"userInput\" placeholder=\"Type a message...\" style=\"width: 80%\" />\n",
              "            <button onclick=\"sendMessage()\">Send</button>\n",
              "        </div>\n",
              "    </div>\n",
              "    <script>\n",
              "        function sendMessage() {\n",
              "            var input = document.getElementById(\"userInput\").value;\n",
              "            if (input.trim() === \"\") return;  // Ignore empty messages\n",
              "            \n",
              "            var chatbox = document.getElementById(\"chatbox\");\n",
              "            chatbox.innerHTML += \"<p class='user-message'>User: \" + input + \"</p>\";  // Append user input\n",
              "\n",
              "            google.colab.kernel.invokeFunction('notebook.chat_with_gemini', [input], {}).then(\n",
              "                (result) => {\n",
              "                    chatbox.innerHTML += \"<p class='ai-message'>NutriAI: \" + result.data[\"text/plain\"] + \"</p>\";  // Append AI response\n",
              "                    chatbox.scrollTop = chatbox.scrollHeight;  // Auto-scroll to latest message\n",
              "                }\n",
              "            );\n",
              "\n",
              "            document.getElementById(\"userInput\").value = \"\";  // Clear input field\n",
              "        }\n",
              "    </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}